import pandas as pd
from sqlalchemy import create_engine, text
import os
import glob
import logging
import configparser
from typing import Optional, Dict, Any
from contextlib import contextmanager
import time
import sys
from dataclasses import dataclass
from functools import lru_cache
import hashlib


@dataclass
class ImportStats:
    """åŒ¯å…¥çµ±è¨ˆè³‡æ–™çµæ§‹"""
    imported_rows: int = 0
    updated_rows: int = 0
    skipped_rows: int = 0
    total_rows: int = 0
    elapsed_time: float = 0.0
    success: bool = False
    error_message: Optional[str] = None
    mode_used: str = "unknown"
    detection_reason: Optional[str] = None
    quality_report: Optional[Dict] = None


class ProgressReporter:
    """é€²åº¦å ±å‘Šå™¨ - åˆ†é›¢æ—¥èªŒå’Œçµ‚ç«¯è¼¸å‡º"""

    def __init__(self, logger: logging.Logger):
        self.logger = logger
        self.console_output = True

    def info(self, message: str, log_only: bool = False):
        """è³‡è¨Šè¨Šæ¯"""
        self.logger.info(message)
        if not log_only and self.console_output:
            print(f"â„¹ï¸  {message}")

    def success(self, message: str, log_only: bool = False):
        """æˆåŠŸè¨Šæ¯"""
        self.logger.info(f"SUCCESS: {message}")
        if not log_only and self.console_output:
            print(f"âœ… {message}")

    def warning(self, message: str, log_only: bool = False):
        """è­¦å‘Šè¨Šæ¯"""
        self.logger.warning(message)
        if not log_only and self.console_output:
            print(f"âš ï¸  {message}")

    def error(self, message: str, log_only: bool = False):
        """éŒ¯èª¤è¨Šæ¯"""
        self.logger.error(message)
        if not log_only and self.console_output:
            print(f"âŒ {message}")

    def progress(self, message: str, log_only: bool = False):
        """é€²åº¦è¨Šæ¯"""
        self.logger.debug(message)
        if not log_only and self.console_output:
            print(f"ğŸ”„ {message}")

    def header(self, title: str, width: int = 80):
        """æ¨™é¡Œ"""
        self.logger.info(f"=== {title} ===")
        if self.console_output:
            print(f"\n{'='*width}")
            print(f"{title:^{width}}")
            print(f"{'='*width}")

    def separator(self, width: int = 80):
        """åˆ†éš”ç¬¦"""
        if self.console_output:
            print(f"{'-'*width}")


class DatabaseConfig:
    """è³‡æ–™åº«é…ç½®é¡"""

    def __init__(self, config_file: str = "config.ini"):
        self.config = configparser.ConfigParser()
        self.config.read(config_file, encoding='utf-8')

        # è®€å–é…ç½®
        self.server = self.config.get('database', 'server')
        self.database = self.config.get('database', 'database')
        self.driver = self.config.get('database', 'driver')

        # å¯é¸çš„èªè­‰è³‡è¨Š
        try:
            self.username = self.config.get('database', 'username')
            self.password = self.config.get('database', 'password')
        except (configparser.NoOptionError, configparser.NoSectionError):
            self.username = None
            self.password = None

    @lru_cache(maxsize=1)
    def get_sqlalchemy_url(self) -> str:
        """ç”Ÿæˆ SQLAlchemy é€£æ¥å­—ä¸²ï¼ˆå¿«å–çµæœï¼‰"""
        if self.username and self.password:
            return (f"mssql+pyodbc://{self.username}:{self.password}@"
                    f"{self.server}/{self.database}?driver={self.driver}")
        else:
            return (f"mssql+pyodbc://@{self.server}/{self.database}"
                    f"?driver={self.driver}&trusted_connection=yes")


class DataCleaner:
    """æ•¸æ“šæ¸…ç†å·¥å…·é¡"""

    # æ¬„ä½å°æ‡‰è¡¨ - ä½¿ç”¨å¸¸æ•¸é¿å…é‡è¤‡å®šç¾©
    COLUMN_MAPPING = {
        'Date': 'date',
        'Open': 'open_price',
        'High': 'high_price',
        'Low': 'low_price',
        'Close': 'close_price',
        'Volume': 'volume',
        'RSI(5)': 'rsi_5',
        'RSI(7)': 'rsi_7',
        'RSI(10)': 'rsi_10',
        'RSI(14)': 'rsi_14',
        'RSI(21)': 'rsi_21',
        'DIF': 'dif',
        'MACD': 'macd',
        'MACD_Histogram': 'macd_histogram',
        'RSV': 'rsv',
        'K': 'k_value',
        'D': 'd_value',
        'J': 'j_value',
        'MA5': 'ma5',
        'MA10': 'ma10',
        'MA20': 'ma20',
        'MA60': 'ma60',
        'EMA12': 'ema12',
        'EMA26': 'ema26',
        'BB_Upper': 'bb_upper',
        'BB_Middle': 'bb_middle',
        'BB_Lower': 'bb_lower',
        'ATR': 'atr',
        'CCI': 'cci',
        'WILLR': 'willr',
        'MOM': 'mom'
    }

    # æ•¸å€¼æ¬„ä½åˆ—è¡¨
    NUMERIC_COLUMNS = [
        'open_price', 'high_price', 'low_price', 'close_price', 'volume',
        'rsi_5', 'rsi_7', 'rsi_10', 'rsi_14', 'rsi_21',
        'dif', 'macd', 'macd_histogram',
        'rsv', 'k_value', 'd_value', 'j_value',
        'ma5', 'ma10', 'ma20', 'ma60', 'ema12', 'ema26',
        'bb_upper', 'bb_middle', 'bb_lower', 'atr', 'cci', 'willr', 'mom'
    ]

    @classmethod
    def clean_dataframe(cls, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """æ¸…ç†å’Œæº–å‚™ DataFrame"""
        # å‰µå»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸå§‹æ•¸æ“š
        df_clean = df.copy()

        # é‡æ–°å‘½åæ¬„ä½
        df_clean = df_clean.rename(columns=cls.COLUMN_MAPPING)

        # æ–°å¢è‚¡ç¥¨ä»£ç¢¼æ¬„ä½
        df_clean['symbol'] = symbol

        # è½‰æ›æ—¥æœŸæ ¼å¼
        df_clean['date'] = pd.to_datetime(df_clean['date'])

        # å„ªåŒ–æ•¸å€¼è½‰æ› - åªè™•ç†å¯¦éš›å­˜åœ¨çš„æ•¸å€¼æ¬„ä½
        existing_numeric_cols = [
            col for col in cls.NUMERIC_COLUMNS if col in df_clean.columns]
        if existing_numeric_cols:
            df_clean[existing_numeric_cols] = (
                df_clean[existing_numeric_cols].apply(
                    pd.to_numeric, errors='coerce'))

        # ç§»é™¤å®Œå…¨ç©ºç™½çš„è¡Œ
        df_clean = df_clean.dropna(subset=['date', 'symbol'])

        return df_clean

    @classmethod
    def validate_data_quality(cls, df: pd.DataFrame) -> Dict[str, Any]:
        """é©—è­‰è³‡æ–™å“è³ª"""
        if df.empty:
            return {
                'total_rows': 0,
                'null_counts': {},
                'duplicate_dates': 0,
                'date_range': {'min': None, 'max': None}
            }

        quality_report = {
            'total_rows': len(df),
            'null_counts': df.isnull().sum().to_dict(),
            'duplicate_dates': (df['date'].duplicated().sum()
                                if 'date' in df.columns else 0),
            'date_range': {
                'min': df['date'].min() if 'date' in df.columns else None,
                'max': df['date'].max() if 'date' in df.columns else None
            }
        }
        return quality_report

    @staticmethod
    def calculate_data_hash(df: pd.DataFrame) -> str:
        """è¨ˆç®—æ•¸æ“šæ¡†çš„é›œæ¹Šå€¼ï¼Œç”¨æ–¼å¿«é€Ÿæ¯”è¼ƒ"""
        if df.empty:
            return ""

        # é¸æ“‡é—œéµæ¬„ä½è¨ˆç®—é›œæ¹Š
        key_columns = ['date', 'close_price', 'volume']
        hash_data = df[key_columns].to_string()
        return hashlib.md5(hash_data.encode()).hexdigest()


class DataComparator:
    """æ•¸æ“šæ¯”è¼ƒå·¥å…·é¡"""

    # æ¯”è¼ƒé–¾å€¼é…ç½®
    COMPARISON_THRESHOLDS = {
        'price_columns': ['open_price', 'high_price',
                          'low_price', 'close_price'],
        'price_threshold': 0.0001,  # 0.01%
        'volume_threshold': 0,  # å®Œå…¨ç²¾ç¢ºæ¯”è¼ƒ
        'rsi_threshold': 0.01,
        'macd_threshold': 0.001,
        'kdj_threshold': 0.01,
        'ma_threshold': 0.0001,
        'bb_threshold': 0.0001,
        'atr_threshold': 0.001,
        'cci_willr_threshold': 0.1,
        'mom_threshold': 0.001,
        'default_threshold': 0.01
    }

    @classmethod
    def get_comparison_threshold(cls, column: str) -> float:
        """æ ¹æ“šæ¬„ä½é¡å‹ç²å–æ¯”è¼ƒé–¾å€¼"""
        if column in cls.COMPARISON_THRESHOLDS['price_columns']:
            return cls.COMPARISON_THRESHOLDS['price_threshold']
        elif column == 'volume':
            return cls.COMPARISON_THRESHOLDS['volume_threshold']
        elif column.startswith('rsi_'):
            return cls.COMPARISON_THRESHOLDS['rsi_threshold']
        elif column in ['dif', 'macd', 'macd_histogram']:
            return cls.COMPARISON_THRESHOLDS['macd_threshold']
        elif column in ['rsv', 'k_value', 'd_value', 'j_value']:
            return cls.COMPARISON_THRESHOLDS['kdj_threshold']
        elif column.startswith('ma') or column.startswith('ema'):
            return cls.COMPARISON_THRESHOLDS['ma_threshold']
        elif column.startswith('bb_'):
            return cls.COMPARISON_THRESHOLDS['bb_threshold']
        elif column == 'atr':
            return cls.COMPARISON_THRESHOLDS['atr_threshold']
        elif column in ['cci', 'willr']:
            return cls.COMPARISON_THRESHOLDS['cci_willr_threshold']
        elif column == 'mom':
            return cls.COMPARISON_THRESHOLDS['mom_threshold']
        else:
            return cls.COMPARISON_THRESHOLDS['default_threshold']

    @classmethod
    def compare_values(cls, file_val: Any, db_val: Any, column: str) -> bool:
        """æ¯”è¼ƒå…©å€‹å€¼æ˜¯å¦æœ‰é¡¯è‘—å·®ç•°"""
        # è™•ç†ç©ºå€¼
        file_val = file_val if pd.notna(file_val) else 0
        db_val = db_val if pd.notna(db_val) else 0

        # ç‰¹æ®Šè™•ç†æˆäº¤é‡ï¼ˆæ•´æ•¸æ¯”è¼ƒï¼‰
        if column == 'volume':
            file_val = int(file_val) if file_val != 0 else 0
            db_val = int(db_val) if db_val != 0 else 0

        threshold = cls.get_comparison_threshold(column)

        # å°æ–¼åƒ¹æ ¼ç›¸é—œæ¬„ä½ï¼Œä½¿ç”¨ç›¸å°é–¾å€¼
        if column in cls.COMPARISON_THRESHOLDS['price_columns']:
            threshold = max(
                abs(db_val * cls.COMPARISON_THRESHOLDS['price_threshold']),
                0.001)

        diff = abs(file_val - db_val)
        return diff > threshold


class StockDataImporter:
    """è‚¡ç¥¨æ•¸æ“šåŒ¯å…¥å™¨ - æ”¯æ´æ™ºèƒ½æ›´æ–°æ¨¡å¼"""

    def __init__(self, config_file: str = "config.ini"):
        # è®€å–é…ç½®
        self.config_parser = configparser.ConfigParser()
        self.config_parser.read(config_file, encoding='utf-8')

        # åˆå§‹åŒ–è³‡æ–™åº«é…ç½®
        self.db_config = DatabaseConfig(config_file)

        # è®€å–åŒ¯å…¥è¨­å®š
        self.batch_size = self.config_parser.getint(
            'import_settings', 'batch_size', fallback=30)
        log_level = self.config_parser.get(
            'import_settings', 'log_level', fallback='INFO')

        # åˆå§‹åŒ–
        self.engine = create_engine(
            self.db_config.get_sqlalchemy_url(),
            pool_pre_ping=True,
            pool_recycle=3600  # 1å°æ™‚å›æ”¶é€£ç·š
        )
        self.logger = self._setup_logger(log_level)
        self.reporter = ProgressReporter(self.logger)

        # è®€å–è·¯å¾‘é…ç½®
        self.output_dir = self.config_parser.get(
            'paths', 'output_directory', fallback='./output')

    def _setup_logger(self, log_level: str = 'INFO') -> logging.Logger:
        """è¨­ç½®æ—¥èªŒè¨˜éŒ„å™¨ - åƒ…è¼¸å‡ºåˆ°æª”æ¡ˆ"""
        logger = logging.getLogger('StockDataImporter')

        # æ¸…é™¤ç¾æœ‰çš„è™•ç†å™¨é¿å…é‡è¤‡
        logger.handlers = []

        # è¨­ç½®æ—¥èªŒç´šåˆ¥
        level = getattr(logging, log_level.upper(), logging.INFO)
        logger.setLevel(level)

        # åªè¨­å®šæª”æ¡ˆè™•ç†å™¨ï¼Œç§»é™¤æ§åˆ¶å°è™•ç†å™¨
        try:
            file_handler = logging.FileHandler(
                'stock_import.log', encoding='utf-8')
            file_formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(file_formatter)
            logger.addHandler(file_handler)
        except Exception as e:
            # å¦‚æœç„¡æ³•å‰µå»ºæ—¥èªŒæª”æ¡ˆï¼Œä½¿ç”¨æ¨™æº–éŒ¯èª¤è¼¸å‡º
            print(f"è­¦å‘Šï¼šç„¡æ³•å‰µå»ºæ—¥èªŒæª”æ¡ˆï¼Œå°‡ä½¿ç”¨æ§åˆ¶å°è¼¸å‡º: {e}", file=sys.stderr)

        return logger

    @contextmanager
    def get_connection(self):
        """ç²å–è³‡æ–™åº«é€£æ¥çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        conn = self.engine.connect()
        try:
            yield conn
        finally:
            conn.close()

    def test_connection(self) -> bool:
        """æ¸¬è©¦è³‡æ–™åº«é€£æ¥"""
        try:
            with self.get_connection() as conn:
                conn.execute(text("SELECT 1"))
            self.reporter.success("è³‡æ–™åº«é€£æ¥æ¸¬è©¦æˆåŠŸ")
            return True
        except Exception as e:
            self.reporter.error(f"è³‡æ–™åº«é€£æ¥æ¸¬è©¦å¤±æ•—: {e}")
            return False

    def create_table_if_not_exists(self):
        """å¦‚æœè¡¨ä¸å­˜åœ¨å‰‡å‰µå»ºè¡¨"""
        create_table_sql = text("""
        IF NOT EXISTS (SELECT * FROM sysobjects
                      WHERE name='stock_data' AND xtype='U')
        BEGIN
            CREATE TABLE stock_data (
                id BIGINT IDENTITY(1,1) PRIMARY KEY,
                symbol NVARCHAR(10) NOT NULL,
                date DATE NOT NULL,
                open_price DECIMAL(18,6),
                high_price DECIMAL(18,6),
                low_price DECIMAL(18,6),
                close_price DECIMAL(18,6),
                volume BIGINT,
                rsi_5 DECIMAL(8,4),
                rsi_7 DECIMAL(8,4),
                rsi_10 DECIMAL(8,4),
                rsi_14 DECIMAL(8,4),
                rsi_21 DECIMAL(8,4),
                dif DECIMAL(10,6),
                macd DECIMAL(10,6),
                macd_histogram DECIMAL(10,6),
                rsv DECIMAL(8,4),
                k_value DECIMAL(8,4),
                d_value DECIMAL(8,4),
                j_value DECIMAL(8,4),
                ma5 DECIMAL(18,6),
                ma10 DECIMAL(18,6),
                ma20 DECIMAL(18,6),
                ma60 DECIMAL(18,6),
                ema12 DECIMAL(18,6),
                ema26 DECIMAL(18,6),
                bb_upper DECIMAL(18,6),
                bb_middle DECIMAL(18,6),
                bb_lower DECIMAL(18,6),
                atr DECIMAL(10,6),
                cci DECIMAL(10,4),
                willr DECIMAL(8,4),
                mom DECIMAL(10,6),
                created_at DATETIME2 DEFAULT GETDATE(),
                updated_at DATETIME2 DEFAULT GETDATE(),
                CONSTRAINT UK_stock_symbol_date UNIQUE (symbol, date)
            );

            -- å‰µå»ºç´¢å¼•
            CREATE NONCLUSTERED INDEX IX_stock_data_symbol_date
                ON stock_data (symbol, date DESC);
            CREATE NONCLUSTERED INDEX IX_stock_data_date
                ON stock_data (date DESC);
            CREATE NONCLUSTERED INDEX IX_stock_data_symbol
                ON stock_data (symbol);
        END
        """)

        try:
            with self.get_connection() as conn:
                conn.execute(create_table_sql)
                conn.commit()
            self.reporter.info("è³‡æ–™è¡¨å’Œç´¢å¼•æª¢æŸ¥/å‰µå»ºå®Œæˆ", log_only=True)
        except Exception as e:
            self.reporter.error(f"å‰µå»ºè³‡æ–™è¡¨å¤±æ•—: {e}")
            raise

    def get_existing_data_info(self, symbol: str) -> Dict[str, Any]:
        """ç²å–å·²å­˜åœ¨æ•¸æ“šçš„è³‡è¨Šï¼ˆå„ªåŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            with self.get_connection() as conn:
                # åŸºæœ¬è³‡è¨ŠæŸ¥è©¢ + å¿«é€Ÿé›œæ¹Šæª¢æŸ¥
                query = text("""
                SELECT
                    COUNT(*) as record_count,
                    MIN(date) as earliest_date,
                    MAX(date) as latest_date,
                    MAX(updated_at) as last_updated,
                    CHECKSUM_AGG(CHECKSUM(CONCAT(CAST(date AS VARCHAR),
                                                 CAST(close_price AS VARCHAR),
                                                 CAST(volume AS VARCHAR))))
                        as data_checksum
                FROM stock_data
                WHERE symbol = :symbol
                """)

                result = conn.execute(query, {"symbol": symbol}).fetchone()

                if result and result[0] > 0:
                    return {
                        'exists': True,
                        'record_count': result[0],
                        'earliest_date': result[1],
                        'latest_date': result[2],
                        'last_updated': result[3],
                        'data_checksum': result[4] if result[4] else 0
                    }
                else:
                    return {'exists': False}

        except Exception as e:
            self.reporter.warning(f"ç²å–å·²å­˜åœ¨æ•¸æ“šè³‡è¨Šå¤±æ•—: {e}", log_only=True)
            return {'exists': False}

    def quick_file_check(self, file_path: str, symbol: str) -> Dict[str, Any]:
        """å¿«é€Ÿæª¢æŸ¥æª”æ¡ˆæ˜¯å¦éœ€è¦è™•ç†ï¼ˆå¹³è¡¡æ•ˆèƒ½èˆ‡æº–ç¢ºæ€§ï¼‰"""
        try:
            # ç²å–è³‡æ–™åº«è³‡è¨Š
            existing_info = self.get_existing_data_info(symbol)

            if not existing_info.get('exists', False):
                return {'needs_processing': True, 'reason': 'æ–°è‚¡ç¥¨éœ€è¦å®Œæ•´åŒ¯å…¥'}

            # è®€å–æª”æ¡ˆçš„å®Œæ•´æ—¥æœŸç¯„åœ
            try:
                df_dates = pd.read_csv(file_path, usecols=['Date'])
                if df_dates.empty:
                    return {'needs_processing': False, 'reason': 'æª”æ¡ˆç‚ºç©º'}

                df_dates['Date'] = pd.to_datetime(df_dates['Date'])
                file_earliest_date = df_dates['Date'].min().date()
                file_latest_date = df_dates['Date'].max().date()
                file_record_count = len(df_dates)

            except Exception as e:
                return {'needs_processing': True, 'reason': f'ç„¡æ³•è®€å–æª”æ¡ˆæ—¥æœŸç¯„åœ: {e}'}

            # ç²å–è³‡æ–™åº«æ—¥æœŸç¯„åœ
            db_earliest_date = existing_info['earliest_date']
            db_latest_date = existing_info['latest_date']
            db_record_count = existing_info.get('record_count', 0)

            if hasattr(db_earliest_date, 'date'):
                db_earliest_date = db_earliest_date.date()
            if hasattr(db_latest_date, 'date'):
                db_latest_date = db_latest_date.date()

            # æª¢æŸ¥æ˜¯å¦æœ‰æ­·å²æ•¸æ“šéœ€è¦åŒ¯å…¥
            has_historical_data = file_earliest_date < db_earliest_date
            has_future_data = file_latest_date > db_latest_date

            # å¦‚æœæœ‰æ­·å²æ•¸æ“šæˆ–æ–°æ•¸æ“šï¼Œéœ€è¦è™•ç†
            if has_historical_data or has_future_data:
                reasons = []
                if has_historical_data:
                    historical_count = len(
                        df_dates[df_dates['Date'].dt.date < db_earliest_date])
                    reasons.append(
                        f"{historical_count} ç­†æ­·å²æ•¸æ“š (æª”æ¡ˆæœ€æ—©: "
                        f"{file_earliest_date}, è³‡æ–™åº«æœ€æ—©: {db_earliest_date})")

                if has_future_data:
                    future_count = len(
                        df_dates[df_dates['Date'].dt.date > db_latest_date])
                    reasons.append(
                        f"{future_count} ç­†æ–°æ•¸æ“š (æª”æ¡ˆæœ€æ–°: {file_latest_date}, "
                        f"è³‡æ–™åº«æœ€æ–°: {db_latest_date})")

                return {
                    'needs_processing': True,
                    'reason': 'ç™¼ç¾ ' + ' å’Œ '.join(reasons),
                    'existing_info': existing_info
                }

            # æª¢æŸ¥è¨˜éŒ„æ•¸æ˜¯å¦é¡¯è‘—ä¸åŒ
            if abs(file_record_count - db_record_count) > 10:  # å…è¨±å°å¹…å·®ç•°
                return {
                    'needs_processing': True,
                    'reason': (f'è¨˜éŒ„æ•¸å·®ç•°éå¤§ (æª”æ¡ˆ: {file_record_count}, '
                               f'è³‡æ–™åº«: {db_record_count})'),
                    'existing_info': existing_info
                }

            # æª¢æŸ¥æª”æ¡ˆä¿®æ”¹æ™‚é–“ï¼ˆèª¿æ•´ç‚ºæ›´å¯¬é¬†çš„æª¢æŸ¥ï¼‰
            file_stat = os.stat(file_path)
            file_mtime = file_stat.st_mtime
            last_updated = existing_info.get('last_updated')

            if last_updated:
                import datetime
                if hasattr(last_updated, 'timestamp'):
                    db_timestamp = last_updated.timestamp()
                else:
                    db_timestamp = last_updated

                # é™ä½æ™‚é–“å·®é–¾å€¼ï¼Œè®“æª”æ¡ˆæ›´å®¹æ˜“è¢«èªç‚ºéœ€è¦æ›´æ–°
                time_diff_hours = (file_mtime - db_timestamp) / 3600

                # å¦‚æœæª”æ¡ˆæ¯”è³‡æ–™åº«æ–°10åˆ†é˜ä»¥ä¸Šï¼Œå°±é€²è¡Œé€²ä¸€æ­¥æª¢æŸ¥
                if time_diff_hours > 0.17:  # 10åˆ†é˜ = 0.17å°æ™‚
                    return {
                        'needs_processing': True,
                        'reason': f'æª”æ¡ˆä¿®æ”¹æ™‚é–“è¼ƒæ–° ({time_diff_hours:.1f}å°æ™‚)ï¼Œéœ€è¦è©³ç´°æª¢æŸ¥',
                        'existing_info': existing_info
                    }

            # æœ€å¾Œçš„ä¿éšªæª¢æŸ¥ï¼šå¦‚æœæ—¥æœŸç¯„åœå®Œå…¨ç›¸åŒä½†ä»Šå¤©æ˜¯å·¥ä½œæ—¥ï¼Œä¹Ÿæª¢æŸ¥ä¸€ä¸‹
            today = datetime.date.today()
            is_weekday = today.weekday() < 5  # 0-4 æ˜¯é€±ä¸€åˆ°é€±äº”

            # å¦‚æœæ˜¯å·¥ä½œæ—¥ä¸”æª”æ¡ˆæœ€æ–°æ—¥æœŸæ˜¯ä»Šå¤©æˆ–æ˜¨å¤©ï¼Œé€²è¡Œé€²ä¸€æ­¥æª¢æŸ¥
            if is_weekday and (file_latest_date >= today
                               - datetime.timedelta(days=1)):
                return {
                    'needs_processing': True,
                    'reason': 'å·¥ä½œæ—¥ä¸”æª”æ¡ˆåŒ…å«æœ€æ–°æ•¸æ“šï¼Œé€²è¡Œè©³ç´°æª¢æŸ¥',
                    'existing_info': existing_info
                }

            # æ‰€æœ‰å¿«é€Ÿæª¢æŸ¥éƒ½é€šéï¼Œå¯èƒ½ä¸éœ€è¦æ›´æ–°
            return {
                'needs_processing': False,
                'reason': (f'å¿«é€Ÿæª¢æŸ¥ï¼šæª”æ¡ˆå¯èƒ½ç„¡é‡å¤§è®ŠåŒ– (æª”æ¡ˆ: {file_earliest_date}'
                           f"~{file_latest_date}, è³‡æ–™åº«: "
                           f"{db_earliest_date}~{db_latest_date})"),
                'existing_info': existing_info
            }

        except Exception as e:
            # å¦‚æœå¿«é€Ÿæª¢æŸ¥å¤±æ•—ï¼Œå›é€€åˆ°å®Œæ•´è™•ç†ä»¥ç¢ºä¿ä¸éºæ¼æ•¸æ“š
            return {'needs_processing': True, 'reason': f'å¿«é€Ÿæª¢æŸ¥å¤±æ•—ï¼Œå°‡é€²è¡Œå®Œæ•´è™•ç†: {e}'}

    def detect_data_changes_optimized(self, df_clean: pd.DataFrame,
                                      existing_info: Dict) -> Dict[str, Any]:
        """å„ªåŒ–ç‰ˆæ•¸æ“šè®ŠåŒ–æª¢æ¸¬ï¼ˆæ¸›å°‘è³‡æ–™åº«æŸ¥è©¢ï¼‰"""
        if not existing_info.get('exists', False):
            return {
                'mode': 'insert_all',
                'data_to_process': df_clean,
                'reason': 'è‚¡ç¥¨ä¸å­˜åœ¨æ–¼è³‡æ–™åº«'
            }

        # ç²å–æª”æ¡ˆå’Œè³‡æ–™åº«çš„æ—¥æœŸç¯„åœ
        file_earliest_date = df_clean['date'].min().date()
        file_latest_date = df_clean['date'].max().date()
        db_earliest_date = existing_info['earliest_date']
        db_latest_date = existing_info['latest_date']

        if hasattr(db_earliest_date, 'date'):
            db_earliest_date = db_earliest_date.date()
        if hasattr(db_latest_date, 'date'):
            db_latest_date = db_latest_date.date()

        # å¿«é€Ÿæª¢æŸ¥ï¼šå¦‚æœæª”æ¡ˆæ—¥æœŸç¯„åœå®Œå…¨åœ¨è³‡æ–™åº«ç¯„åœå…§ï¼Œä¸”è¨˜éŒ„æ•¸ç›¸åŒ
        if (file_earliest_date >= db_earliest_date and
            file_latest_date <= db_latest_date and
                len(df_clean) == existing_info.get('record_count', 0)):

            # ä½¿ç”¨å¿«é€Ÿé›œæ¹Šæ¯”è¼ƒï¼ˆå¦‚æœæ”¯æ´ï¼‰
            if 'data_checksum' in existing_info:
                file_key_data = df_clean[[
                    'date', 'close_price', 'volume']].copy()
                file_key_data['date'] = file_key_data['date'].dt.strftime(
                    '%Y-%m-%d')
                file_hash = hash(str(file_key_data.values.tolist()))

                # ç°¡å–®çš„é›œæ¹Šæ¯”è¼ƒï¼ˆä¸æ˜¯å®Œå…¨æº–ç¢ºï¼Œä½†å¯ä»¥å¿«é€Ÿæ’é™¤å¤§éƒ¨åˆ†ä¸è®Šçš„æƒ…æ³ï¼‰
                file_hash_mod = abs(file_hash) % 1000000
                db_hash_mod = abs(existing_info['data_checksum']) % 1000000
                if file_hash_mod == db_hash_mod:
                    return {
                        'mode': 'skip',
                        'data_to_process': pd.DataFrame(),
                        'reason': 'å¿«é€Ÿé›œæ¹Šæª¢æŸ¥ï¼šæ•¸æ“šç„¡è®ŠåŒ–'
                    }

        # æª¢æŸ¥æ˜¯å¦æœ‰æ–°çš„æ­·å²æ•¸æ“šæˆ–æœªä¾†æ•¸æ“š
        historical_data = df_clean[df_clean['date'].dt.date < db_earliest_date]
        future_data = df_clean[df_clean['date'].dt.date > db_latest_date]

        # å¦‚æœæœ‰æ­·å²æ•¸æ“šæˆ–æœªä¾†æ•¸æ“šéœ€è¦æ’å…¥
        if not historical_data.empty or not future_data.empty:
            new_data_frames = []
            reasons = []

            if not historical_data.empty:
                new_data_frames.append(historical_data)
                reasons.append(f"{len(historical_data)} ç­†æ­·å²æ•¸æ“š")

            if not future_data.empty:
                new_data_frames.append(future_data)
                reasons.append(f"{len(future_data)} ç­†æ–°æ•¸æ“š")

            if new_data_frames:
                combined_new_data = pd.concat(
                    new_data_frames, ignore_index=True)
                return {
                    'mode': 'incremental',
                    'data_to_process': combined_new_data,
                    'reason': "ç™¼ç¾ " + " å’Œ ".join(reasons)
                }

        # åªå°å¯èƒ½è®ŠåŒ–çš„æœŸé–“é€²è¡Œè©³ç´°æ¯”è¼ƒï¼ˆé™åˆ¶ç¯„åœï¼‰
        overlap_start = max(file_earliest_date, db_earliest_date)
        overlap_end = min(file_latest_date, db_latest_date)

        if overlap_start <= overlap_end:
            # åªæª¢æŸ¥æœ€è¿‘30å¤©çš„æ•¸æ“šè®ŠåŒ–ï¼ˆæ¸›å°‘æ¯”è¼ƒç¯„åœï¼‰
            import datetime
            recent_date = max(
                overlap_end - datetime.timedelta(days=30), overlap_start)

            overlap_file_data = df_clean[
                (df_clean['date'].dt.date >= recent_date) &
                (df_clean['date'].dt.date <= overlap_end)
            ]

            # é™åˆ¶æ¯”è¼ƒçš„è¡Œæ•¸
            if not overlap_file_data.empty and len(overlap_file_data) <= 100:
                changed_rows = self._compare_with_database(
                    overlap_file_data, df_clean['symbol'].iloc[0])

                if not changed_rows.empty:
                    return {
                        'mode': 'update_changed',
                        'data_to_process': changed_rows,
                        'reason': f'æœ€è¿‘30å¤©å…§æª¢æ¸¬åˆ° {len(changed_rows)} ç­†æ•¸æ“šæœ‰è®ŠåŒ–'
                    }

        return {
            'mode': 'skip',
            'data_to_process': pd.DataFrame(),
            'reason': 'æœ‰é™ç¯„åœæª¢æŸ¥ï¼šæ•¸æ“šç„¡é‡å¤§è®ŠåŒ–'
        }

    def _compare_with_database(self, df: pd.DataFrame,
                               stock_symbol: str) -> pd.DataFrame:
        """èˆ‡è³‡æ–™åº«æ•¸æ“šé€²è¡Œè©³ç´°æ¯”è¼ƒ"""
        try:
            if df.empty:
                return pd.DataFrame()

            with self.get_connection() as conn:
                # ç²å–è³‡æ–™åº«ä¸­å°æ‡‰æ—¥æœŸçš„æ•¸æ“š
                dates = df['date'].dt.date.tolist()
                date_str = ','.join([f"'{date}'" for date in dates])

                query = text(f"""
                SELECT date, open_price, high_price, low_price, close_price,
                       volume, rsi_5, rsi_7, rsi_10, rsi_14, rsi_21,
                       dif, macd, macd_histogram,
                             rsv, k_value, d_value, j_value,
                       ma5, ma10, ma20, ma60, ema12, ema26,
                       bb_upper, bb_middle, bb_lower, atr, cci, willr, mom
                FROM stock_data
                WHERE symbol = :symbol AND date IN ({date_str})
                """)

                db_result = conn.execute(
                    query, {"symbol": stock_symbol}).fetchall()

                if not db_result:
                    return df  # å¦‚æœè³‡æ–™åº«æ²’æœ‰æ•¸æ“šï¼Œè¿”å›æ‰€æœ‰æª”æ¡ˆæ•¸æ“š

                # è½‰æ›è³‡æ–™åº«çµæœç‚ºå­—å…¸
                db_data = {}
                for row in db_result:
                    date_key = row[0] if hasattr(row[0], 'date') else row[0]
                    if hasattr(date_key, 'date'):
                        date_key = date_key.date()

                    db_data[date_key] = {
                        col: (float(val) if val is not None else 0)
                        for col, val in zip(DataCleaner.NUMERIC_COLUMNS,
                                            row[1:])
                    }

                # æ¯”è¼ƒæ•¸æ“šä¸¦æ‰¾å‡ºè®ŠåŒ–
                changed_rows = []
                for _, row in df.iterrows():
                    date_key = row['date'].date()
                    if date_key in db_data:
                        db_row = db_data[date_key]

                        # æª¢æŸ¥æ˜¯å¦æœ‰é¡¯è‘—å·®ç•°
                        has_changes = any(
                            DataComparator.compare_values(
                                row.get(col, 0), db_row.get(col, 0), col)
                            for col in DataCleaner.NUMERIC_COLUMNS
                            if col in row.index
                        )

                        if has_changes:
                            changed_rows.append(row)

                return (pd.DataFrame(changed_rows) if changed_rows
                        else pd.DataFrame())

        except Exception as e:
            self.logger.warning(f"æ•¸æ“šæ¯”è¼ƒå¤±æ•—: {e}")
            return df  # æ¯”è¼ƒå¤±æ•—æ™‚è¿”å›æ‰€æœ‰æ•¸æ“šä»¥ç¢ºä¿æ›´æ–°

    def import_single_file(self, file_path: str, symbol: Optional[str] = None,
                           update_mode: str = 'smart') -> ImportStats:
        """åŒ¯å…¥å–®ä¸€ CSV æª”æ¡ˆï¼ˆå„ªåŒ–ç‰ˆæœ¬ï¼‰"""
        start_time = time.time()
        result = ImportStats()

        try:
            # æå–è‚¡ç¥¨ä»£ç¢¼
            if symbol is None:
                symbol = os.path.basename(file_path).replace('.csv', '')

            filename = os.path.basename(file_path)
            self.reporter.progress(f"è™•ç†æª”æ¡ˆ {filename} ({symbol})")

            # æ™ºèƒ½æ¨¡å¼ä¸‹å…ˆé€²è¡Œå¿«é€Ÿæª¢æŸ¥
            if update_mode == 'smart':
                quick_check = self.quick_file_check(file_path, symbol)

                # å¦‚æœå¿«é€Ÿæª¢æŸ¥ç™¼ç¾ä¸éœ€è¦è™•ç†ï¼Œææ—©è¿”å›
                if not quick_check['needs_processing']:
                    result.success = True
                    result.mode_used = 'skip'
                    result.detection_reason = quick_check['reason']
                    result.elapsed_time = time.time() - start_time

                    # ä»éœ€è¦è®€å–æª”æ¡ˆä¾†è¨ˆç®—è·³éçš„è¡Œæ•¸
                    try:
                        df_sample = pd.read_csv(file_path, nrows=1)
                        if not df_sample.empty:
                            # å¿«é€Ÿè¨ˆç®—ç¸½è¡Œæ•¸
                            with open(file_path, 'r', encoding='utf-8') as f:
                                result.total_rows = sum(
                                    1 for _ in f) - 1  # æ¸›å»æ¨™é¡Œè¡Œ
                            result.skipped_rows = result.total_rows
                    except (pd.errors.EmptyDataError, pd.errors.ParserError,
                            FileNotFoundError, UnicodeDecodeError, OSError):
                        result.total_rows = 0
                        result.skipped_rows = 0

                    print("   ğŸ“‹ æ¨¡å¼: è·³éè™•ç† | å¾…è™•ç†: 0 ç­†")
                    print(f"   ğŸ’¡ åŸå› : {result.detection_reason}")
                    self.reporter.success(f"{filename}: å¿«é€Ÿæª¢æŸ¥ - æ•¸æ“šç„¡è®ŠåŒ–ï¼Œè·³éè™•ç†")
                    return result

            # è®€å– CSVï¼ˆåªæœ‰åœ¨éœ€è¦æ™‚æ‰è®€å–å®Œæ•´æª”æ¡ˆï¼‰
            try:
                df = pd.read_csv(file_path)
            except Exception as e:
                result.error_message = f"è®€å–æª”æ¡ˆå¤±æ•—: {e}"
                self.reporter.error(f"è®€å–æª”æ¡ˆ {filename} å¤±æ•—: {e}")
                return result

            result.total_rows = len(df)

            if df.empty:
                result.error_message = "æª”æ¡ˆæ²’æœ‰æ•¸æ“š"
                self.reporter.warning(f"æª”æ¡ˆ {filename} æ²’æœ‰æ•¸æ“š")
                return result

            # æ¸…ç†è³‡æ–™
            df_clean = DataCleaner.clean_dataframe(df, symbol)

            # è³‡æ–™å“è³ªæª¢æŸ¥
            result.quality_report = DataCleaner.validate_data_quality(df_clean)

            # æ ¹æ“šæ¨¡å¼æ±ºå®šè™•ç†æ–¹å¼
            if update_mode == 'smart':
                # å¦‚æœå¿«é€Ÿæª¢æŸ¥å·²ç¶“æä¾›äº†è³‡æ–™åº«è³‡è¨Šï¼Œä½¿ç”¨å®ƒ
                existing_info = quick_check.get('existing_info')
                if not existing_info:
                    existing_info = self.get_existing_data_info(symbol)

                # ä½¿ç”¨å„ªåŒ–çš„æª¢æ¸¬æ–¹æ³•
                change_detection = self.detect_data_changes_optimized(
                    df_clean, existing_info)
                actual_mode = change_detection['mode']
                data_to_process = change_detection['data_to_process']
                result.detection_reason = change_detection['reason']
            else:
                # éæ™ºèƒ½æ¨¡å¼ï¼Œä½¿ç”¨åŸæœ‰æ–¹æ³•
                existing_info = self.get_existing_data_info(symbol)
                change_detection = self.detect_data_changes_optimized(
                    df_clean, existing_info)
                actual_mode = change_detection['mode']
                data_to_process = change_detection['data_to_process']
                result.detection_reason = f"ä½¿ç”¨æŒ‡å®šæ¨¡å¼: {update_mode}"

            result.mode_used = actual_mode

            # é¡¯ç¤ºè™•ç†æ¨¡å¼å’ŒåŸå› 
            mode_descriptions = {
                'insert_all': 'æ–°å¢æ’å…¥',
                'incremental': 'å¢é‡æ›´æ–°',
                'update_changed': 'å·®ç•°æ›´æ–°',
                'update_all': 'å®Œæ•´æ›´æ–°',
                'skip': 'è·³éè™•ç†'
            }
            mode_desc = mode_descriptions.get(actual_mode, actual_mode)

            print(f"   ğŸ“‹ æ¨¡å¼: {mode_desc} | å¾…è™•ç†: {len(data_to_process):,} ç­†")
            print(f"   ğŸ’¡ åŸå› : {result.detection_reason}")

            # å¦‚æœæ˜¯è·³éæ¨¡å¼ï¼Œç›´æ¥è¿”å›
            if actual_mode == 'skip':
                result.success = True
                result.skipped_rows = len(df_clean)
                result.elapsed_time = time.time() - start_time
                self.reporter.success(f"{filename}: æ•¸æ“šç„¡è®ŠåŒ–ï¼Œè·³éè™•ç†")
                return result

            # åŸ·è¡ŒåŒ¯å…¥
            import_result = self._execute_import(
                data_to_process, symbol, actual_mode)

            result.imported_rows = import_result.get('imported_rows', 0)
            result.updated_rows = import_result.get('updated_rows', 0)
            result.skipped_rows = import_result.get('skipped_rows', 0)

            # è¨ˆç®—è·³éçš„è¡Œæ•¸
            if actual_mode in ['incremental', 'update_changed']:
                result.skipped_rows += len(df_clean) - len(data_to_process)

            result.success = (result.imported_rows >
                              0 or result.updated_rows > 0)
            result.elapsed_time = time.time() - start_time

            # é¡¯ç¤ºçµæœ
            if result.success:
                summary = (f"æ–°å¢ {result.imported_rows:,} | "
                           f"æ›´æ–° {result.updated_rows:,} | "
                           f"è·³é {result.skipped_rows:,} | "
                           f"{result.elapsed_time:.2f}ç§’")
                self.reporter.success(f"{filename}: {summary}")
            else:
                self.reporter.warning(f"{filename}: è™•ç†å¤±æ•—")

        except Exception as e:
            result.error_message = str(e)
            result.elapsed_time = time.time() - start_time
            self.reporter.error(f"è™•ç†æª”æ¡ˆ {filename} å¤±æ•—: {e}")

        return result

    def _execute_import(self, df: pd.DataFrame, symbol: str,
                        mode: str) -> Dict[str, int]:
        """åŸ·è¡Œå¯¦éš›çš„åŒ¯å…¥æ“ä½œ"""
        if df.empty:
            return {'imported_rows': 0, 'updated_rows': 0, 'skipped_rows': 0}

        if mode == 'replace':
            return self._replace_all_data(df, symbol)
        elif mode in ['incremental', 'insert_all']:
            return self._incremental_update(df, symbol)
        elif mode in ['update_changed', 'update_all', 'update']:
            return self._update_all_data(df, symbol)
        else:  # insert mode
            return self._insert_new_data(df, symbol)

    def _replace_all_data(self, df: pd.DataFrame,
                          symbol: str) -> Dict[str, int]:
        """æ›¿æ›æ¨¡å¼ï¼šåˆªé™¤èˆŠæ•¸æ“šï¼Œæ’å…¥æ–°æ•¸æ“š"""
        result = {'imported_rows': 0, 'updated_rows': 0, 'skipped_rows': 0}

        try:
            with self.get_connection() as conn:
                # åˆªé™¤èˆŠæ•¸æ“š
                delete_query = text(
                    "DELETE FROM stock_data WHERE symbol = :symbol")
                conn.execute(delete_query, {"symbol": symbol})

                # æ’å…¥æ–°æ•¸æ“š
                df_clean = df.where(pd.notnull(df), None)
                df_clean.to_sql(
                    name='stock_data',
                    con=self.engine,
                    if_exists='append',
                    index=False,
                    method='multi'
                )

                conn.commit()
                result['imported_rows'] = len(df)
                self.logger.info(f"æ›¿æ›æ¨¡å¼ï¼šåˆªé™¤èˆŠæ•¸æ“šä¸¦æ’å…¥ {len(df)} ç­†æ–°æ•¸æ“š")

        except Exception as e:
            self.logger.error(f"æ›¿æ›æ¨¡å¼å¤±æ•—: {e}")
            raise

        return result

    def _incremental_update(self, df: pd.DataFrame,
                            symbol: str) -> Dict[str, int]:
        """å¢é‡æ›´æ–°æ¨¡å¼ï¼šæ’å…¥æ–°æ•¸æ“š"""
        result = {'imported_rows': 0, 'updated_rows': 0, 'skipped_rows': 0}

        try:
            df_clean = df.where(pd.notnull(df), None)
            df_clean.to_sql(
                name='stock_data',
                con=self.engine,
                if_exists='append',
                index=False,
                method='multi'
            )
            result['imported_rows'] = len(df)

        except Exception as e:
            self.logger.error(f"å¢é‡æ›´æ–°å¤±æ•—: {e}")
            # å¦‚æœæ‰¹æ¬¡æ’å…¥å¤±æ•—ï¼Œæ”¹ç”¨ UPSERT
            result = self._upsert_batch_by_batch(df, symbol)

        return result

    def _update_all_data(self, df: pd.DataFrame,
                         symbol: str) -> Dict[str, int]:
        """æ›´æ–°æ¨¡å¼ï¼šä½¿ç”¨ MERGE èªå¥é€²è¡Œ UPSERT"""
        return self._upsert_batch_by_batch(df, symbol)

    def _insert_new_data(self, df: pd.DataFrame,
                         symbol: str) -> Dict[str, int]:
        """æ’å…¥æ¨¡å¼ï¼šç›´æ¥æ’å…¥æ–°æ•¸æ“š"""
        result = {'imported_rows': 0, 'updated_rows': 0, 'skipped_rows': 0}

        try:
            df_clean = df.where(pd.notnull(df), None)
            df_clean.to_sql(
                name='stock_data',
                con=self.engine,
                if_exists='append',
                index=False,
                method='multi'
            )
            result['imported_rows'] = len(df)

        except Exception as e:
            self.logger.warning(f"æ‰¹æ¬¡æ’å…¥å¤±æ•—ï¼Œæ”¹ç”¨ UPSERT æ¨¡å¼: {e}")
            result = self._upsert_batch_by_batch(df, symbol)

        return result

    def _upsert_batch_by_batch(self, df: pd.DataFrame) -> Dict[str, int]:
        """é€ç­†é€²è¡Œ UPSERT æ“ä½œ"""
        result = {'imported_rows': 0, 'updated_rows': 0, 'skipped_rows': 0}

        # æº–å‚™ MERGE SQL
        merge_sql = text("""
        MERGE stock_data AS target
        USING (SELECT :symbol as symbol, :date as date) AS source
        ON (target.symbol = source.symbol AND target.date = source.date)
        WHEN MATCHED THEN
            UPDATE SET
                open_price = :open_price, high_price = :high_price,
                low_price = :low_price, close_price = :close_price,
                volume = :volume, rsi_5 = :rsi_5, rsi_7 = :rsi_7,
                rsi_10 = :rsi_10, rsi_14 = :rsi_14, rsi_21 = :rsi_21,
                dif = :dif, macd = :macd, macd_histogram = :macd_histogram,
                rsv = :rsv, k_value = :k_value, d_value = :d_value,
                j_value = :j_value, ma5 = :ma5, ma10 = :ma10,
                ma20 = :ma20, ma60 = :ma60, ema12 = :ema12, ema26 = :ema26,
                bb_upper = :bb_upper, bb_middle = :bb_middle,
                bb_lower = :bb_lower, atr = :atr, cci = :cci,
                willr = :willr, mom = :mom, updated_at = GETDATE()
        WHEN NOT MATCHED THEN
            INSERT (symbol, date, open_price, high_price, low_price,
                         close_price, volume,
                         rsi_5, rsi_7, rsi_10, rsi_14, rsi_21, dif, macd,
                   macd_histogram, rsv, k_value, d_value, j_value, ma5, ma10,
                   ma20, ma60, ema12, ema26, bb_upper, bb_middle, bb_lower,
                   atr, cci, willr, mom)
            VALUES (:symbol, :date, :open_price, :high_price, :low_price,
                   :close_price, :volume, :rsi_5, :rsi_7, :rsi_10, :rsi_14,
                   :rsi_21, :dif, :macd, :macd_histogram, :rsv, :k_value,
                   :d_value, :j_value, :ma5, :ma10, :ma20, :ma60, :ema12,
                   :ema26, :bb_upper, :bb_middle, :bb_lower, :atr, :cci,
                   :willr, :mom)
        OUTPUT $action;
        """)

        with self.get_connection() as conn:
            for _, row in df.iterrows():
                try:
                    # æº–å‚™åƒæ•¸ï¼Œå°‡ NaN è½‰æ›ç‚º None
                    params = row.to_dict()
                    for key, value in params.items():
                        if pd.isna(value):
                            params[key] = None

                    merge_result = conn.execute(merge_sql, params)
                    action = merge_result.fetchone()

                    if action and action[0] == 'INSERT':
                        result['imported_rows'] += 1
                    elif action and action[0] == 'UPDATE':
                        result['updated_rows'] += 1

                except Exception as row_error:
                    self.logger.debug(
                        f"è™•ç†è¡Œå¤±æ•— {row['symbol']} {row['date']}: {row_error}")
                    result['skipped_rows'] += 1

            conn.commit()

        return result

    def import_directory(self, directory_path: Optional[str] = None,
                         update_mode: str = 'smart') -> Dict[str, ImportStats]:
        """æ‰¹é‡åŒ¯å…¥ç›®éŒ„ä¸­çš„æ‰€æœ‰ CSV æª”æ¡ˆ"""
        if directory_path is None:
            directory_path = self.output_dir

        if not os.path.exists(directory_path):
            self.reporter.error(f"ç›®éŒ„ä¸å­˜åœ¨: {directory_path}")
            return {}

        csv_files = glob.glob(os.path.join(directory_path, "*.csv"))

        if not csv_files:
            self.reporter.warning("ç›®éŒ„ä¸­æ²’æœ‰æ‰¾åˆ° CSV æª”æ¡ˆ")
            return {}

        self.reporter.header("è‚¡ç¥¨æ•¸æ“šåŒ¯å…¥ç¨‹åº")
        self.reporter.info(f"ç›®éŒ„: {directory_path}")
        self.reporter.info(f"æ‰¾åˆ° {len(csv_files)} å€‹æª”æ¡ˆï¼Œä½¿ç”¨ {update_mode} æ¨¡å¼")

        # ç¢ºä¿è³‡æ–™è¡¨å­˜åœ¨
        self.create_table_if_not_exists()

        self.reporter.separator()

        results = {}
        success_count = 0
        total_imported = 0
        total_updated = 0

        for i, file_path in enumerate(csv_files, 1):
            file_name = os.path.basename(file_path)
            print(f"\nğŸ“ [{i}/{len(csv_files)}] {file_name}")

            result = self.import_single_file(
                file_path, update_mode=update_mode)
            results[file_name] = result

            if result.success:
                success_count += 1
                total_imported += result.imported_rows
                total_updated += result.updated_rows

        # é¡¯ç¤ºç¸½çµ
        self.reporter.separator()
        self.reporter.header("åŒ¯å…¥çµæœæ‘˜è¦")

        print("\nğŸ“Š çµæœçµ±è¨ˆ:")
        print(f"   âœ… æˆåŠŸ: {success_count} å€‹æª”æ¡ˆ")
        print(f"   âŒ å¤±æ•—: {len(csv_files) - success_count} å€‹æª”æ¡ˆ")
        print(f"   ğŸ“ˆ ç¸½æ–°å¢: {total_imported:,} ç­†")
        print(f"   ğŸ”„ ç¸½æ›´æ–°: {total_updated:,} ç­†")

        return results

    def get_import_statistics(self) -> Dict[str, Any]:
        """ç²å–åŒ¯å…¥çµ±è¨ˆè³‡è¨Š"""
        try:
            with self.get_connection() as conn:
                stats_query = text("""
                SELECT COUNT(*) as total_records,
                    COUNT(DISTINCT symbol) as unique_symbols,
                    MIN(date) as earliest_date,
                    MAX(date) as latest_date,
                    MAX(created_at) as last_import_time
                FROM stock_data
                """)

                result = conn.execute(stats_query).fetchone()

                symbols_query = text("""
                SELECT symbol, COUNT(*) as record_count,
                       MIN(date) as start_date, MAX(date) as end_date
                FROM stock_data
                GROUP BY symbol
                ORDER BY record_count DESC
                """)

                symbols_result = conn.execute(symbols_query).fetchall()

                return {
                    'total_records': result[0] if result else 0,
                    'unique_symbols': result[1] if result else 0,
                    'date_range': {
                        'earliest': result[2] if result else None,
                        'latest': result[3] if result else None
                    },
                    'last_import': result[4] if result else None,
                    'symbols': [
                        {
                            'symbol': row[0],
                            'records': row[1],
                            'start_date': row[2],
                            'end_date': row[3]
                        } for row in symbols_result
                    ]
                }

        except Exception as e:
            self.logger.error(f"ç²å–çµ±è¨ˆè³‡è¨Šå¤±æ•—: {e}")
            return {}


def print_import_summary(results: Dict[str, ImportStats],
                         reporter: ProgressReporter):
    """æ‰“å°åŒ¯å…¥æ‘˜è¦"""
    reporter.separator()
    reporter.header("åŒ¯å…¥çµæœæ‘˜è¦")

    successful_files = [f for f, r in results.items() if r.success]
    failed_files = [f for f, r in results.items() if not r.success]

    print("\nğŸ“Š çµæœçµ±è¨ˆ:")
    print(f"   âœ… æˆåŠŸ: {len(successful_files)} å€‹æª”æ¡ˆ")
    print(f"   âŒ å¤±æ•—: {len(failed_files)} å€‹æª”æ¡ˆ")

    if successful_files:
        print("\nğŸ“ˆ æˆåŠŸè™•ç†çš„æª”æ¡ˆ:")
        total_new = sum(r.imported_rows for r in results.values())
        total_updated = sum(r.updated_rows for r in results.values())
        total_time = sum(r.elapsed_time for r in results.values())

        for filename in successful_files:
            result = results[filename]
            print(f"   â€¢ {filename}")
            print(
                f"     æ–°å¢: {result.imported_rows:,} | æ›´æ–°: "
                f"{result.updated_rows:,} | è·³é: {result.skipped_rows:,}")
            print(
                f"     æ¨¡å¼: {result.mode_used} | è€—æ™‚: "
                f"{result.elapsed_time:.2f}ç§’")

        print("\nğŸ¯ ç¸½è¨ˆ:")
        print(f"   æ–°å¢è³‡æ–™: {total_new:,} ç­†")
        print(f"   æ›´æ–°è³‡æ–™: {total_updated:,} ç­†")
        print(f"   ç¸½è€—æ™‚: {total_time:.2f} ç§’")

    if failed_files:
        print("\nâŒ å¤±æ•—çš„æª”æ¡ˆ:")
        for filename in failed_files:
            result = results[filename]
            print(f"   â€¢ {filename}: {result.error_message or 'æœªçŸ¥éŒ¯èª¤'}")


def print_database_statistics(stats: Dict[str, Any],
                              reporter: ProgressReporter):
    """æ‰“å°è³‡æ–™åº«çµ±è¨ˆè³‡è¨Š"""
    if not stats:
        return

    reporter.separator()
    reporter.header("è³‡æ–™åº«çµ±è¨ˆè³‡è¨Š")

    print("\nğŸ“ˆ æ•´é«”çµ±è¨ˆ:")
    print(f"   ç¸½è¨˜éŒ„æ•¸: {stats['total_records']:,}")
    print(f"   è‚¡ç¥¨æ•¸é‡: {stats['unique_symbols']}")
    print(
        f"   æ—¥æœŸç¯„åœ: {stats['date_range']['earliest']} ~ "
        f"{stats['date_range']['latest']}")
    print(f"   æœ€å¾Œæ›´æ–°: {stats['last_import']}")

    if stats['symbols']:
        print("\nğŸ“‹ å„è‚¡ç¥¨çµ±è¨ˆ:")
        for symbol_info in stats['symbols'][:10]:
            print(f"   â€¢ {symbol_info['symbol']}: "
                  f"{symbol_info['records']:,} ç­† "
                  f"({symbol_info['start_date']} ~ {symbol_info['end_date']})")

        if len(stats['symbols']) > 10:
            print(f"   ... é‚„æœ‰ {len(stats['symbols']) - 10} å€‹è‚¡ç¥¨")


def main():
    """ä¸»ç¨‹å¼"""
    try:
        # å‰µå»ºåŒ¯å…¥å™¨
        importer = StockDataImporter()

        # æ¸¬è©¦é€£æ¥
        if not importer.test_connection():
            importer.reporter.error("è³‡æ–™åº«é€£æ¥å¤±æ•—ï¼Œç¨‹å¼çµæŸ")
            return

        # åŒ¯å…¥æ•¸æ“š
        results = importer.import_directory(update_mode='smart')

        # é¡¯ç¤ºçµæœæ‘˜è¦
        print_import_summary(results, importer.reporter)

        # é¡¯ç¤ºè³‡æ–™åº«çµ±è¨ˆ
        stats = importer.get_import_statistics()
        print_database_statistics(stats, importer.reporter)

        # çµæŸè¨Šæ¯
        importer.reporter.separator()
        importer.reporter.success("ç¨‹åºåŸ·è¡Œå®Œæˆï¼")
        print("\nğŸ“ è©³ç´°æ—¥èªŒè«‹æŸ¥çœ‹: stock_import.log")

    except Exception as e:
        print(f"\nâŒ ç¨‹å¼åŸ·è¡ŒéŒ¯èª¤: {e}")
        logging.error(f"ä¸»ç¨‹å¼éŒ¯èª¤: {e}", exc_info=True)


if __name__ == "__main__":
    main()
